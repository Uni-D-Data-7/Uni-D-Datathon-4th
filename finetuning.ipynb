{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nxpYP7-zR84P",
        "eFT2M14mSmYw",
        "77GFZ3J8R84X",
        "U-mfoJmuR84Y",
        "ImJjWj0T64oU",
        "JnpTzUqd67aa",
        "HLcKQnMn6f2v",
        "vWmhJQuzDyy9",
        "D7jgXeid_11f",
        "-o7sxQn8o5Dj"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxpYP7-zR84P"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e-XQqO-wSCJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5a0c60-3156-4858-933f-26f1d603a8c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o4nuy8gLR84T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision.transforms import CenterCrop, Resize\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path Setting"
      ],
      "metadata": {
        "id": "eFT2M14mSmYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEhTJGWLSsTi",
        "outputId": "5f9da22c-778f-49ed-f899-42961cc17f93"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpJJXRVzR84V"
      },
      "source": [
        "# Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GgKnpGLER84V"
      },
      "outputs": [],
      "source": [
        "CFG = {\n",
        "    'IMG_SIZE':224,\n",
        "    'EPOCHS':15,\n",
        "    'LEARNING_RATE':3e-4,\n",
        "    'BATCH_SIZE':16,\n",
        "    'SEED':42\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "825k_sRIR84W"
      },
      "source": [
        "# Fixed RandomSeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MZ4nyJm2R84W"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED']) # Seed 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77GFZ3J8R84X"
      },
      "source": [
        "# Data Pre-processing (한 번만 실행하면 됨)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya-zEn3WR84X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def Preprocess(base_dir, num_images=None):\n",
        "    # 처리된 이미지가 저장될 디렉토리 설정\n",
        "    processed_dir = os.path.join('/content/drive/MyDrive/Processed', base_dir.split('/')[-1])\n",
        "    clean_dir = os.path.join(processed_dir, 'clean')\n",
        "    noisy_dir = os.path.join(processed_dir, 'noisy')\n",
        "\n",
        "    os.makedirs(clean_dir, exist_ok=True)\n",
        "    os.makedirs(noisy_dir, exist_ok=True)\n",
        "\n",
        "    # GT 디렉토리 (Label 데이터) 찾기\n",
        "    source_dirs = []\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        for dir_name in dirs:\n",
        "            if 'GT' in dir_name:\n",
        "                source_dirs.append(os.path.join(root, dir_name))\n",
        "\n",
        "    if not source_dirs:\n",
        "        raise ValueError(\"No directory containing 'GT' found\")\n",
        "\n",
        "    # GT가 포함된 디렉토리에서 모든 clean 이미지를 복사하여 저장\n",
        "    for source_dir in source_dirs:\n",
        "        for filename in os.listdir(source_dir):\n",
        "            if filename.endswith('.jpg'):\n",
        "                shutil.copy(os.path.join(source_dir, filename), os.path.join(clean_dir, filename))\n",
        "\n",
        "    # '밝은 조도'와 '어두운 조도' 폴더에서 동일한 갯수의 noisy 이미지를 가져오기\n",
        "    lighting_conditions = ['밝은 조도', '어두운 조도']\n",
        "    for condition in lighting_conditions:\n",
        "        condition_dir = os.path.join(base_dir, condition)\n",
        "        if not os.path.exists(condition_dir):\n",
        "            print(f\"{condition} 폴더가 없습니다.\")\n",
        "            continue\n",
        "\n",
        "        for root, dirs, files in os.walk(condition_dir):\n",
        "            for dir_name in dirs:\n",
        "                if 'GT' in dir_name:\n",
        "                    continue  # GT 디렉토리는 이미 처리되었으므로 건너뜀\n",
        "\n",
        "                current_dir = os.path.join(root, dir_name)\n",
        "                image_files = [f for f in os.listdir(current_dir) if f.endswith('.jpg')]\n",
        "                print(f'{len(image_files)} files in {current_dir}')\n",
        "                # 이미지 수가 지정되면 그 수만큼 랜덤으로 선택, 아니면 전체를 선택\n",
        "                if num_images is not None and num_images < len(image_files):\n",
        "                    image_files = random.sample(image_files, num_images)\n",
        "\n",
        "                # 선택된 이미지를 noisy 폴더에 복사\n",
        "                for filename in image_files:\n",
        "                    shutil.copy(os.path.join(current_dir, filename), os.path.join(noisy_dir, f\"{condition}_{filename}\"))\n",
        "\n",
        "    print('preprocessing done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "SyDZeOg5R84X",
        "outputId": "24b3adfb-15e7-4540-d1c3-633fbe416a3e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-544b1fdef527>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_base_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mPreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_base_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_base_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-cb9b33446ef6>\u001b[0m in \u001b[0;36mPreprocess\u001b[0;34m(base_dir, num_images)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# GT 디렉토리 (Label 데이터) 찾기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msource_dirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdir_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'GT'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;31m# above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_walk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopdown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Recurse into sub-directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;31m# above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_walk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopdown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Recurse into sub-directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;31m# above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_walk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopdown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Recurse into sub-directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# Note that scandir is global in this module due\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# to earlier import-*.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mscandir_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0monerror\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data_dir = '/content/drive/MyDrive/event (1)'\n",
        "training_base_dir = os.path.join(data_dir, 'Training')\n",
        "validation_base_dir = os.path.join(data_dir, 'Validation')\n",
        "\n",
        "Preprocess(training_base_dir)\n",
        "Preprocess(validation_base_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-mfoJmuR84Y"
      },
      "source": [
        "# CustomDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fRXw316SR84Y"
      },
      "outputs": [],
      "source": [
        "# 원하는 유형의 데이터셋을 로드할 수 있도록 변경함.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, clean_image_paths, noisy_image_paths, image_types=None, transform=None):\n",
        "        self.clean_image_paths = [os.path.join(clean_image_paths, x) for x in os.listdir(clean_image_paths)]\n",
        "        self.noisy_image_paths = [os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
        "        self.transform = transform\n",
        "        self.center_crop = CenterCrop(1080)\n",
        "        self.resize = Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
        "        self.image_types = image_types  # 추가된 파라미터로 원하는 이미지 유형 리스트\n",
        "\n",
        "        # Create a list of (noisy, clean) pairs\n",
        "        self.noisy_clean_pairs = self._create_noisy_clean_pairs()\n",
        "\n",
        "    def _create_noisy_clean_pairs(self):\n",
        "        clean_to_noisy = {}\n",
        "        for clean_path in self.clean_image_paths:\n",
        "            clean_id = '_'.join(os.path.basename(clean_path).split('_')[:-1])\n",
        "            clean_to_noisy[clean_id] = clean_path\n",
        "\n",
        "        noisy_clean_pairs = []\n",
        "        for noisy_path in self.noisy_image_paths:\n",
        "            noisy_id = '_'.join(os.path.basename(noisy_path).split('_')[:-1])\n",
        "            image_type = os.path.basename(noisy_path).split('_')[-1].replace(\".jpg\", \"\")  # 이미지 유형 추출\n",
        "\n",
        "            # 이미지 유형이 주어졌을 경우 필터링\n",
        "            if self.image_types is None or image_type in self.image_types:\n",
        "                if noisy_id in clean_to_noisy:\n",
        "                    clean_path = clean_to_noisy[noisy_id]\n",
        "                    noisy_clean_pairs.append((noisy_path, clean_path))\n",
        "\n",
        "        return noisy_clean_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.noisy_clean_pairs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        noisy_image_path, clean_image_path = self.noisy_clean_pairs[index]\n",
        "\n",
        "        noisy_image = Image.open(noisy_image_path).convert(\"RGB\")\n",
        "        clean_image = Image.open(clean_image_path).convert(\"RGB\")\n",
        "\n",
        "        # Central Crop and Resize\n",
        "        noisy_image = self.center_crop(noisy_image)\n",
        "        clean_image = self.center_crop(clean_image)\n",
        "        noisy_image = self.resize(noisy_image)\n",
        "        clean_image = self.resize(clean_image)\n",
        "\n",
        "        if self.transform:\n",
        "            noisy_image = self.transform(noisy_image)\n",
        "            clean_image = self.transform(clean_image)\n",
        "\n",
        "        return noisy_image, clean_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_S45stCR84Y"
      },
      "source": [
        "# Model Define"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline"
      ],
      "metadata": {
        "id": "ImJjWj0T64oU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3jAkDq4R84Y"
      },
      "outputs": [],
      "source": [
        "class MDTA(nn.Module):\n",
        "    def __init__(self, channels, num_heads):\n",
        "        super(MDTA, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
        "        self.qkv_conv = nn.Conv2d(channels * 3, channels * 3, kernel_size=3, padding=1, groups=channels * 3, bias=False)\n",
        "        self.project_out = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        q, k, v = self.qkv_conv(self.qkv(x)).chunk(3, dim=1)\n",
        "\n",
        "        q = q.reshape(b, self.num_heads, -1, h * w)\n",
        "        k = k.reshape(b, self.num_heads, -1, h * w)\n",
        "        v = v.reshape(b, self.num_heads, -1, h * w)\n",
        "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1)\n",
        "\n",
        "        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)\n",
        "        out = self.project_out(torch.matmul(attn, v).reshape(b, -1, h, w))\n",
        "        return out\n",
        "\n",
        "\n",
        "class GDFN(nn.Module):\n",
        "    def __init__(self, channels, expansion_factor):\n",
        "        super(GDFN, self).__init__()\n",
        "\n",
        "        hidden_channels = int(channels * expansion_factor)\n",
        "        self.project_in = nn.Conv2d(channels, hidden_channels * 2, kernel_size=1, bias=False)\n",
        "        self.conv = nn.Conv2d(hidden_channels * 2, hidden_channels * 2, kernel_size=3, padding=1,\n",
        "                              groups=hidden_channels * 2, bias=False)\n",
        "        self.project_out = nn.Conv2d(hidden_channels, channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2 = self.conv(self.project_in(x)).chunk(2, dim=1)\n",
        "        x = self.project_out(F.gelu(x1) * x2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, channels, num_heads, expansion_factor):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(channels)\n",
        "        self.attn = MDTA(channels, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(channels)\n",
        "        self.ffn = GDFN(channels, expansion_factor)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = x + self.attn(self.norm1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
        "                          .contiguous().reshape(b, c, h, w))\n",
        "        x = x + self.ffn(self.norm2(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
        "                         .contiguous().reshape(b, c, h, w))\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(DownSample, self).__init__()\n",
        "        self.body = nn.Sequential(nn.Conv2d(channels, channels // 2, kernel_size=3, padding=1, bias=False),\n",
        "                                  nn.PixelUnshuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(UpSample, self).__init__()\n",
        "        self.body = nn.Sequential(nn.Conv2d(channels, channels * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                  nn.PixelShuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "\n",
        "class Restormer(nn.Module):\n",
        "    def __init__(self, num_blocks=[4, 6, 6, 8], num_heads=[1, 2, 4, 8], channels=[24, 48, 96, 192], num_refinement=4, expansion_factor=2.66):\n",
        "\n",
        "        super(Restormer, self).__init__()\n",
        "\n",
        "        self.embed_conv = nn.Conv2d(3, channels[0], kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.encoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(\n",
        "            num_ch, num_ah, expansion_factor) for _ in range(num_tb)]) for num_tb, num_ah, num_ch in\n",
        "                                       zip(num_blocks, num_heads, channels)])\n",
        "\n",
        "        # the number of down sample or up sample == the number of encoder - 1\n",
        "        self.downs = nn.ModuleList([DownSample(num_ch) for num_ch in channels[:-1]])\n",
        "        self.ups = nn.ModuleList([UpSample(num_ch) for num_ch in list(reversed(channels))[:-1]])\n",
        "\n",
        "        # the number of reduce block == the number of decoder - 1\n",
        "        self.reduces = nn.ModuleList([nn.Conv2d(channels[i], channels[i - 1], kernel_size=1, bias=False)\n",
        "                                      for i in reversed(range(2, len(channels)))])\n",
        "\n",
        "        # the number of decoder == the number of encoder - 1\n",
        "        self.decoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(channels[2], num_heads[2], expansion_factor)\n",
        "                                                       for _ in range(num_blocks[2])])])\n",
        "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[1], expansion_factor)\n",
        "                                             for _ in range(num_blocks[1])]))\n",
        "\n",
        "        # the channel of last one is not change\n",
        "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
        "                                             for _ in range(num_blocks[0])]))\n",
        "\n",
        "        self.refinement = nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
        "                                          for _ in range(num_refinement)])\n",
        "        self.output = nn.Conv2d(channels[1], 3, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fo = self.embed_conv(x)\n",
        "        out_enc1 = self.encoders[0](fo)\n",
        "        out_enc2 = self.encoders[1](self.downs[0](out_enc1))\n",
        "        out_enc3 = self.encoders[2](self.downs[1](out_enc2))\n",
        "        out_enc4 = self.encoders[3](self.downs[2](out_enc3))\n",
        "\n",
        "        out_dec3 = self.decoders[0](self.reduces[0](torch.cat([self.ups[0](out_enc4), out_enc3], dim=1)))\n",
        "        out_dec2 = self.decoders[1](self.reduces[1](torch.cat([self.ups[1](out_dec3), out_enc2], dim=1)))\n",
        "        fd = self.decoders[2](torch.cat([self.ups[2](out_dec2), out_enc1], dim=1))\n",
        "        fr = self.refinement(fd)\n",
        "        out = self.output(fr) + x\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained"
      ],
      "metadata": {
        "id": "JnpTzUqd67aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Restormer: Efficient Transformer for High-Resolution Image Restoration\n",
        "## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang\n",
        "## https://arxiv.org/abs/2111.09881\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pdb import set_trace as stx\n",
        "import numbers\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Layer Norm\n",
        "\n",
        "def to_3d(x):\n",
        "    return rearrange(x, 'b c h w -> b (h w) c')\n",
        "\n",
        "def to_4d(x,h,w):\n",
        "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
        "\n",
        "class BiasFree_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(BiasFree_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
        "\n",
        "class WithBias_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(WithBias_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = x.mean(-1, keepdim=True)\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim, LayerNorm_type):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        if LayerNorm_type =='BiasFree':\n",
        "            self.body = BiasFree_LayerNorm(dim)\n",
        "        else:\n",
        "            self.body = WithBias_LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[-2:]\n",
        "        return to_4d(self.body(to_3d(x)), h, w)\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Gated-Dconv Feed-Forward Network (GDFN)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        hidden_features = int(dim*ffn_expansion_factor)\n",
        "\n",
        "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
        "\n",
        "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
        "\n",
        "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
        "        x = F.gelu(x1) * x2\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Multi-DConv Head Transposed Self-Attention (MDTA)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, bias):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
        "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
        "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "\n",
        "        qkv = self.qkv_dwconv(self.qkv(x))\n",
        "        q,k,v = qkv.chunk(3, dim=1)\n",
        "\n",
        "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "\n",
        "        q = torch.nn.functional.normalize(q, dim=-1)\n",
        "        k = torch.nn.functional.normalize(k, dim=-1)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = (attn @ v)\n",
        "\n",
        "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
        "\n",
        "        out = self.project_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
        "        self.attn = Attention(dim, num_heads, bias)\n",
        "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
        "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.ffn(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Overlapped image patch embedding with 3x3 Conv\n",
        "class OverlapPatchEmbed(nn.Module):\n",
        "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
        "        super(OverlapPatchEmbed, self).__init__()\n",
        "\n",
        "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Resizing modules\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, n_feat):\n",
        "        super(Downsample, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                  nn.PixelUnshuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, n_feat):\n",
        "        super(Upsample, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                  nn.PixelShuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "##########################################################################\n",
        "##---------- Restormer -----------------------\n",
        "class PretrainedRestormer(nn.Module):\n",
        "    def __init__(self,\n",
        "        inp_channels=3,\n",
        "        out_channels=3,\n",
        "        dim = 48,\n",
        "        num_blocks = [4,6,6,8],\n",
        "        num_refinement_blocks = 4,\n",
        "        heads = [1,2,4,8],\n",
        "        ffn_expansion_factor = 2.66,\n",
        "        bias = False,\n",
        "        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
        "        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n",
        "    ):\n",
        "\n",
        "        super(PretrainedRestormer, self).__init__()\n",
        "\n",
        "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
        "\n",
        "        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "\n",
        "        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
        "        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
        "\n",
        "        self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n",
        "        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
        "\n",
        "        self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n",
        "        self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])\n",
        "\n",
        "        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n",
        "        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n",
        "        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
        "\n",
        "\n",
        "        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n",
        "        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n",
        "        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
        "\n",
        "        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
        "\n",
        "        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "\n",
        "        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n",
        "\n",
        "        #### For Dual-Pixel Defocus Deblurring Task ####\n",
        "        self.dual_pixel_task = dual_pixel_task\n",
        "        if self.dual_pixel_task:\n",
        "            self.skip_conv = nn.Conv2d(dim, int(dim*2**1), kernel_size=1, bias=bias)\n",
        "        ###########################\n",
        "\n",
        "        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "\n",
        "    def forward(self, inp_img):\n",
        "\n",
        "        inp_enc_level1 = self.patch_embed(inp_img)\n",
        "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
        "\n",
        "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
        "        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
        "\n",
        "        inp_enc_level3 = self.down2_3(out_enc_level2)\n",
        "        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n",
        "\n",
        "        inp_enc_level4 = self.down3_4(out_enc_level3)\n",
        "        latent = self.latent(inp_enc_level4)\n",
        "\n",
        "        inp_dec_level3 = self.up4_3(latent)\n",
        "        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
        "        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
        "        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n",
        "\n",
        "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
        "        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
        "        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
        "        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n",
        "\n",
        "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
        "        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
        "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
        "\n",
        "        out_dec_level1 = self.refinement(out_dec_level1)\n",
        "\n",
        "        #### For Dual-Pixel Defocus Deblurring Task ####\n",
        "        if self.dual_pixel_task:\n",
        "            out_dec_level1 = out_dec_level1 + self.skip_conv(inp_enc_level1)\n",
        "            out_dec_level1 = self.output(out_dec_level1)\n",
        "        ###########################\n",
        "        else:\n",
        "            out_dec_level1 = self.output(out_dec_level1) + inp_img\n",
        "\n",
        "\n",
        "        return out_dec_level1\n"
      ],
      "metadata": {
        "id": "-ysy_OTqwfwZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rFScjN8R84Z"
      },
      "source": [
        "# Train(Ablation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline"
      ],
      "metadata": {
        "id": "HLcKQnMn6f2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arZ19ll4R84Z",
        "outputId": "3c4aba9c-c4ca-4e15-e293-d7f76e278b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Size: 13921\n",
            "Total Parameters: 6676132\n",
            "Epoch 1/1, MSE Loss: 0.1670, Lr: 0.00000000\n",
            "1epoch 훈련 소요 시간: 0시간 14분 49초\n",
            "1epoch 모델 저장 완료\n",
            "훈련 소요 시간: 0시간 14분 53초\n"
          ]
        }
      ],
      "source": [
        "# 시작 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "def load_img(filepath):\n",
        "    img = cv2.imread(filepath)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# 데이터셋 경로\n",
        "noisy_image_paths = '/content/drive/MyDrive/event (1)/Training/noisy'\n",
        "clean_image_paths = '/content/drive/MyDrive/event (1)/Training/clean'\n",
        "\n",
        "# 데이터셋 로드 및 전처리\n",
        "train_transform = Compose([\n",
        "    ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 커스텀 데이터셋 인스턴스 생성\n",
        "train_dataset = CustomDataset(clean_image_paths, noisy_image_paths, transform=train_transform)\n",
        "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
        "\n",
        "# 데이터 로더 설정\n",
        "num_cores = os.cpu_count()\n",
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=int(num_cores/2), shuffle=True)\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Restormer 모델 인스턴스 생성 및 GPU로 이동\n",
        "model = Restormer().to(device)\n",
        "\n",
        "# 손실 함수와 최적화 알고리즘 설정\n",
        "optimizer = optim.AdamW(model.parameters(), lr = CFG['LEARNING_RATE'], weight_decay=1e-4)\n",
        "criterion = nn.L1Loss()\n",
        "scaler = GradScaler()\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
        "\n",
        "# 모델의 파라미터 수 계산\n",
        "total_parameters = count_parameters(model)\n",
        "print(\"Total Parameters:\", total_parameters)\n",
        "\n",
        "# 모델 학습\n",
        "model.train()\n",
        "best_loss = 1000\n",
        "\n",
        "for epoch in range(CFG['EPOCHS']):\n",
        "    model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    mse_running_loss = 0.0\n",
        "\n",
        "    for noisy_images, clean_images in train_loader:\n",
        "        noisy_images = noisy_images.to(device)\n",
        "        clean_images = clean_images.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(noisy_images)\n",
        "            mse_loss = criterion(outputs, clean_images)\n",
        "\n",
        "        scaler.scale(mse_loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        mse_running_loss += mse_loss.item() * noisy_images.size(0)\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    minutes = int(epoch_time // 60)\n",
        "    seconds = int(epoch_time % 60)\n",
        "    hours = int(minutes // 60)\n",
        "    minutes = int(minutes % 60)\n",
        "\n",
        "    mse_epoch_loss = mse_running_loss / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, MSE Loss: {mse_epoch_loss:.4f}, Lr: {current_lr:.8f}\")\n",
        "    print(f\"1epoch 훈련 소요 시간: {hours}시간 {minutes}분 {seconds}초\")\n",
        "\n",
        "    if mse_epoch_loss < best_loss:\n",
        "        best_loss = mse_epoch_loss\n",
        "        torch.save(model.state_dict(), 'best_Restormer.pth')\n",
        "        print(f\"{epoch+1}epoch 모델 저장 완료\")\n",
        "\n",
        "# 종료 시간 기록\n",
        "end_time = time.time()\n",
        "\n",
        "# 소요 시간 계산\n",
        "training_time = end_time - start_time\n",
        "minutes = int(training_time // 60)\n",
        "seconds = int(training_time % 60)\n",
        "hours = int(minutes // 60)\n",
        "minutes = int(minutes % 60)\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"훈련 소요 시간: {hours}시간 {minutes}분 {seconds}초\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning - PSNR Ver 1"
      ],
      "metadata": {
        "id": "u6qu5MYe4yCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# 시작 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "# 데이터셋 경로\n",
        "noisy_image_paths = '/content/drive/MyDrive/event/Training/noisy'\n",
        "clean_image_paths = '/content/drive/MyDrive/event/Training/clean'\n",
        "\n",
        "# 데이터 전처리 설정\n",
        "train_transform = Compose([\n",
        "    transforms.Resize((128, 128)),  # 이미지 크기 줄이기\n",
        "    ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 데이터셋 및 데이터 로더 설정\n",
        "train_dataset = CustomDataset(clean_image_paths, noisy_image_paths, transform=train_transform)\n",
        "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
        "\n",
        "# 데이터 로더 설정\n",
        "num_cores = os.cpu_count()\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], num_workers=int(num_cores/2), shuffle=True)\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "pretrained_model = PretrainedRestormer().to(device)\n",
        "\n",
        "# 사전학습된 가중치 로드\n",
        "def load_pretrained_checkpoint(filepath, model):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    state_dict = checkpoint.get('params') or checkpoint.get('state_dict') or checkpoint\n",
        "\n",
        "    # 입력 채널이 불일치하는 경우 첫 번째 레이어 가중치 수정\n",
        "    if model.patch_embed.proj.weight.shape[1] != state_dict['patch_embed.proj.weight'].shape[1]:\n",
        "        print(\"Adjusting input channels to match model configuration\")\n",
        "        original_weight = state_dict['patch_embed.proj.weight']\n",
        "        new_weight = original_weight[:, :3, :, :]  # 첫 3채널만 가져오기 (RGB)\n",
        "        state_dict['patch_embed.proj.weight'] = new_weight\n",
        "\n",
        "    # 가중치 로드\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Checkpoint loaded successfully\")\n",
        "\n",
        "pretrained_checkpoint_path = \"./pretrained_models/single_image_defocus_deblurring.pth\"\n",
        "load_pretrained_checkpoint(pretrained_checkpoint_path, pretrained_model)\n",
        "\n",
        "# 손실 함수, 옵티마이저, 학습률 스케줄러 설정\n",
        "def psnr_loss(pred, target, max_val=1.0):\n",
        "    mse = torch.mean((pred - target) ** 2)\n",
        "    return -10 * torch.log10(mse + 1e-10) + max_val\n",
        "\n",
        "criterion = lambda pred, target: nn.L1Loss()(pred, target) - psnr_loss(pred, target)\n",
        "# criterion = nn.L1Loss()\n",
        "\n",
        "optimizer = optim.AdamW(pretrained_model.parameters(), lr=CFG['LEARNING_RATE'], weight_decay=1e-4)\n",
        "scaler = GradScaler()  # AMP용 스케일러\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
        "\n",
        "# 파라미터 수 출력\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_parameters = count_parameters(pretrained_model)\n",
        "print(\"Total Parameters:\", total_parameters)\n",
        "\n",
        "# 모델 파인튜닝\n",
        "pretrained_model.train()\n",
        "best_loss = float('inf')\n",
        "\n",
        "# GPU 메모리 릴리스\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory has been released.\")\n",
        "\n",
        "for epoch in range(CFG['EPOCHS']):\n",
        "    pretrained_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for noisy_images, clean_images in train_loader:\n",
        "        noisy_images = noisy_images.to(device)\n",
        "        clean_images = clean_images.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 자동 혼합 정밀도(AMP) 사용\n",
        "        with autocast():\n",
        "            outputs = pretrained_model(noisy_images)\n",
        "            loss = criterion(outputs, clean_images)\n",
        "\n",
        "        # 역전파 및 최적화\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(pretrained_model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * noisy_images.size(0)\n",
        "\n",
        "    scheduler.step()  # 학습률 스케줄러 업데이트\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    # 에포크 소요 시간 계산\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    minutes = int(epoch_time // 60)\n",
        "    seconds = int(epoch_time % 60)\n",
        "    hours = int(minutes // 60)\n",
        "    minutes = int(minutes % 60)\n",
        "\n",
        "    # 에포크 손실 및 학습률 출력\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Loss: {epoch_loss:.4f}, LR: {current_lr:.8f}\")\n",
        "    print(f\"Epoch Time: {hours}h {minutes}m {seconds}s\")\n",
        "\n",
        "    # 최상의 모델 저장\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(pretrained_model.state_dict(), 'fine_tuned_PSNRloss.pth')\n",
        "        print(f\"Best model saved at epoch {epoch+1}\")\n",
        "\n",
        "# 전체 학습 소요 시간 계산\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "minutes = int(training_time // 60)\n",
        "seconds = int(training_time % 60)\n",
        "hours = int(minutes // 60)\n",
        "minutes = int(minutes % 60)\n",
        "\n",
        "# 전체 훈련 시간 출력\n",
        "print(f\"Total Training Time: {hours}h {minutes}m {seconds}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilm6cRUc4wRG",
        "outputId": "6788ba8e-9598-41d1-f444-f3a69213e1d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Size: 13921\n",
            "Checkpoint loaded successfully\n",
            "Total Parameters: 26126644\n",
            "GPU memory has been released.\n",
            "Epoch 1/15, Loss: -14.1498, LR: 0.00029672\n",
            "Epoch Time: 0h 7m 19s\n",
            "Best model saved at epoch 1\n",
            "Epoch 2/15, Loss: -15.6837, LR: 0.00028703\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 2\n",
            "Epoch 3/15, Loss: -16.6854, LR: 0.00027135\n",
            "Epoch Time: 0h 5m 42s\n",
            "Best model saved at epoch 3\n",
            "Epoch 4/15, Loss: -17.4790, LR: 0.00025037\n",
            "Epoch Time: 0h 5m 42s\n",
            "Best model saved at epoch 4\n",
            "Epoch 5/15, Loss: -18.1463, LR: 0.00022500\n",
            "Epoch Time: 0h 5m 42s\n",
            "Best model saved at epoch 5\n",
            "Epoch 6/15, Loss: -18.8069, LR: 0.00019635\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 6\n",
            "Epoch 7/15, Loss: -19.4515, LR: 0.00016568\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 7\n",
            "Epoch 8/15, Loss: -19.8453, LR: 0.00013432\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 8\n",
            "Epoch 9/15, Loss: -20.4983, LR: 0.00010365\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 9\n",
            "Epoch 10/15, Loss: -21.3030, LR: 0.00007500\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 10\n",
            "Epoch 11/15, Loss: -21.8340, LR: 0.00004963\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 11\n",
            "Epoch 12/15, Loss: -22.2766, LR: 0.00002865\n",
            "Epoch Time: 0h 5m 42s\n",
            "Best model saved at epoch 12\n",
            "Epoch 13/15, Loss: -22.6289, LR: 0.00001297\n",
            "Epoch Time: 0h 5m 42s\n",
            "Best model saved at epoch 13\n",
            "Epoch 14/15, Loss: -22.8566, LR: 0.00000328\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 14\n",
            "Epoch 15/15, Loss: -22.9857, LR: 0.00000000\n",
            "Epoch Time: 0h 5m 42s\n",
            "Best model saved at epoch 15\n",
            "Total Training Time: 1h 27m 32s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning - PSNR Ver 2"
      ],
      "metadata": {
        "id": "vWmhJQuzDyy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# 시작 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "# 데이터셋 경로\n",
        "noisy_image_paths = '/content/drive/MyDrive/event/Training/noisy'\n",
        "clean_image_paths = '/content/drive/MyDrive/event/Training/clean'\n",
        "\n",
        "# 데이터 전처리 설정\n",
        "train_transform = Compose([\n",
        "    transforms.Resize((128, 128)),  # 이미지 크기 줄이기\n",
        "    ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 데이터셋 및 데이터 로더 설정\n",
        "train_dataset = CustomDataset(clean_image_paths, noisy_image_paths, transform=train_transform)\n",
        "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
        "\n",
        "# 데이터 로더 설정\n",
        "num_cores = os.cpu_count()\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], num_workers=int(num_cores/2), shuffle=True)\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "pretrained_model = PretrainedRestormer().to(device)\n",
        "\n",
        "# 사전학습된 가중치 로드\n",
        "def load_pretrained_checkpoint(filepath, model):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    state_dict = checkpoint.get('params') or checkpoint.get('state_dict') or checkpoint\n",
        "\n",
        "    # 입력 채널이 불일치하는 경우 첫 번째 레이어 가중치 수정\n",
        "    if model.patch_embed.proj.weight.shape[1] != state_dict['patch_embed.proj.weight'].shape[1]:\n",
        "        print(\"Adjusting input channels to match model configuration\")\n",
        "        original_weight = state_dict['patch_embed.proj.weight']\n",
        "        new_weight = original_weight[:, :3, :, :]  # 첫 3채널만 가져오기 (RGB)\n",
        "        state_dict['patch_embed.proj.weight'] = new_weight\n",
        "\n",
        "    # 가중치 로드\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Checkpoint loaded successfully\")\n",
        "\n",
        "pretrained_checkpoint_path = \"./pretrained_models/single_image_defocus_deblurring.pth\"\n",
        "load_pretrained_checkpoint(pretrained_checkpoint_path, pretrained_model)\n",
        "\n",
        "# 손실 함수, 옵티마이저, 학습률 스케줄러 설정\n",
        "def psnr_loss(pred, target, max_val=1.0):\n",
        "    # PSNR 계산\n",
        "    mse = torch.mean((pred - target) ** 2)\n",
        "    psnr = 10 * torch.log10((max_val ** 2) / (mse + 1e-10))\n",
        "\n",
        "    # PSNR을 음수로 변환하여 손실 함수로 사용\n",
        "    return -psnr\n",
        "\n",
        "criterion = psnr_loss\n",
        "# criterion = nn.L1Loss()\n",
        "\n",
        "optimizer = optim.AdamW(pretrained_model.parameters(), lr=CFG['LEARNING_RATE'], weight_decay=1e-4)\n",
        "scaler = GradScaler()  # AMP용 스케일러\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
        "\n",
        "# 파라미터 수 출력\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_parameters = count_parameters(pretrained_model)\n",
        "print(\"Total Parameters:\", total_parameters)\n",
        "\n",
        "# 모델 파인튜닝\n",
        "pretrained_model.train()\n",
        "best_loss = float('inf')\n",
        "\n",
        "# GPU 메모리 릴리스\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory has been released.\")\n",
        "\n",
        "for epoch in range(CFG['EPOCHS']):\n",
        "    pretrained_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for noisy_images, clean_images in train_loader:\n",
        "        noisy_images = noisy_images.to(device)\n",
        "        clean_images = clean_images.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 자동 혼합 정밀도(AMP) 사용\n",
        "        with autocast():\n",
        "            outputs = pretrained_model(noisy_images)\n",
        "            loss = criterion(outputs, clean_images)\n",
        "\n",
        "        # 역전파 및 최적화\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(pretrained_model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * noisy_images.size(0)\n",
        "\n",
        "    scheduler.step()  # 학습률 스케줄러 업데이트\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    # 에포크 소요 시간 계산\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    minutes = int(epoch_time // 60)\n",
        "    seconds = int(epoch_time % 60)\n",
        "    hours = int(minutes // 60)\n",
        "    minutes = int(minutes % 60)\n",
        "\n",
        "    # 에포크 손실 및 학습률 출력\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Loss: {epoch_loss:.4f}, LR: {current_lr:.8f}\")\n",
        "    print(f\"Epoch Time: {hours}h {minutes}m {seconds}s\")\n",
        "\n",
        "    # 최상의 모델 저장\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(pretrained_model.state_dict(), 'fine_tuned_PSNRloss2_20epoch.pth')\n",
        "        print(f\"Best model saved at epoch {epoch+1}\")\n",
        "\n",
        "# 전체 학습 소요 시간 계산\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "minutes = int(training_time // 60)\n",
        "seconds = int(training_time % 60)\n",
        "hours = int(minutes // 60)\n",
        "minutes = int(minutes % 60)\n",
        "\n",
        "# 전체 훈련 시간 출력\n",
        "print(f\"Total Training Time: {hours}h {minutes}m {seconds}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmOPhPzLDqP8",
        "outputId": "e8459848-0711-4e5a-a81a-0bf890d7150d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Size: 13921\n",
            "Checkpoint loaded successfully\n",
            "Total Parameters: 26126644\n",
            "GPU memory has been released.\n",
            "Epoch 1/20, Loss: -13.0708, LR: 0.00029815\n",
            "Epoch Time: 0h 7m 7s\n",
            "Best model saved at epoch 1\n",
            "Epoch 2/20, Loss: -14.8423, LR: 0.00029266\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 2\n",
            "Epoch 3/20, Loss: -15.7329, LR: 0.00028365\n",
            "Epoch Time: 0h 5m 44s\n",
            "Best model saved at epoch 3\n",
            "Epoch 4/20, Loss: -16.6085, LR: 0.00027135\n",
            "Epoch Time: 0h 5m 44s\n",
            "Best model saved at epoch 4\n",
            "Epoch 5/20, Loss: -17.0950, LR: 0.00025607\n",
            "Epoch Time: 0h 5m 44s\n",
            "Best model saved at epoch 5\n",
            "Epoch 6/20, Loss: -17.8065, LR: 0.00023817\n",
            "Epoch Time: 0h 5m 44s\n",
            "Best model saved at epoch 6\n",
            "Epoch 7/20, Loss: -18.5695, LR: 0.00021810\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 7\n",
            "Epoch 8/20, Loss: -19.0824, LR: 0.00019635\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 8\n",
            "Epoch 9/20, Loss: -19.6512, LR: 0.00017347\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 9\n",
            "Epoch 10/20, Loss: -19.7329, LR: 0.00015000\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 10\n",
            "Epoch 11/20, Loss: -19.9721, LR: 0.00012653\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 11\n",
            "Epoch 12/20, Loss: -20.5930, LR: 0.00010365\n",
            "Epoch Time: 0h 5m 44s\n",
            "Best model saved at epoch 12\n",
            "Epoch 13/20, Loss: -21.0045, LR: 0.00008190\n",
            "Epoch Time: 0h 5m 44s\n",
            "Best model saved at epoch 13\n",
            "Epoch 14/20, Loss: -21.4584, LR: 0.00006183\n",
            "Epoch Time: 0h 5m 42s\n",
            "Best model saved at epoch 14\n",
            "Epoch 15/20, Loss: -21.9562, LR: 0.00004393\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 15\n",
            "Epoch 16/20, Loss: -22.2261, LR: 0.00002865\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 16\n",
            "Epoch 17/20, Loss: -22.5957, LR: 0.00001635\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 17\n",
            "Epoch 18/20, Loss: -22.8862, LR: 0.00000734\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 18\n",
            "Epoch 19/20, Loss: -23.0075, LR: 0.00000185\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 19\n",
            "Epoch 20/20, Loss: -23.1186, LR: 0.00000000\n",
            "Epoch Time: 0h 5m 43s\n",
            "Best model saved at epoch 20\n",
            "Total Training Time: 1h 56m 7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning - Layer Freeze(Train only Attention)"
      ],
      "metadata": {
        "id": "D7jgXeid_11f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# 시작 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "# 데이터셋 경로\n",
        "noisy_image_paths = '/content/drive/MyDrive/event/Training/noisy'\n",
        "clean_image_paths = '/content/drive/MyDrive/event/Training/clean'\n",
        "\n",
        "# 데이터 전처리 설정\n",
        "train_transform = Compose([\n",
        "    transforms.Resize((128, 128)),  # 이미지 크기 줄이기\n",
        "    ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 데이터셋 및 데이터 로더 설정\n",
        "train_dataset = CustomDataset(clean_image_paths, noisy_image_paths, transform=train_transform)\n",
        "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
        "\n",
        "# 데이터 로더 설정\n",
        "num_cores = os.cpu_count()\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], num_workers=int(num_cores/2), shuffle=True)\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "pretrained_model = PretrainedRestormer().to(device)\n",
        "\n",
        "# 사전학습된 가중치 로드\n",
        "def load_pretrained_checkpoint(filepath, model):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    state_dict = checkpoint.get('params') or checkpoint.get('state_dict') or checkpoint\n",
        "\n",
        "    # 입력 채널이 불일치하는 경우 첫 번째 레이어 가중치 수정\n",
        "    if model.patch_embed.proj.weight.shape[1] != state_dict['patch_embed.proj.weight'].shape[1]:\n",
        "        print(\"Adjusting input channels to match model configuration\")\n",
        "        original_weight = state_dict['patch_embed.proj.weight']\n",
        "        new_weight = original_weight[:, :3, :, :]  # 첫 3채널만 가져오기 (RGB)\n",
        "        state_dict['patch_embed.proj.weight'] = new_weight\n",
        "\n",
        "    # 가중치 로드\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Checkpoint loaded successfully\")\n",
        "\n",
        "pretrained_checkpoint_path = \"./pretrained_models/single_image_defocus_deblurring.pth\"\n",
        "load_pretrained_checkpoint(pretrained_checkpoint_path, pretrained_model)\n",
        "\n",
        "# 모든 파라미터 고정 (학습되지 않도록 설정)\n",
        "for param in pretrained_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 디코더의 마지막 레이어만 학습 가능하도록 설정\n",
        "for param in pretrained_model.decoder_level1.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# GDFN 및 MDTA 모듈만 학습 가능하도록 설정\n",
        "for module in pretrained_model.modules():\n",
        "    if isinstance(module, FeedForward) or isinstance(module, Attention):\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# 손실 함수 설정\n",
        "def psnr_loss(pred, target, max_val=1.0):\n",
        "    mse = torch.mean((pred - target) ** 2)\n",
        "    return -10 * torch.log10(mse + 1e-10) + max_val\n",
        "\n",
        "criterion = lambda pred, target: nn.L1Loss()(pred, target) - psnr_loss(pred, target)\n",
        "\n",
        "# 필요한 부분만 업데이트하도록 옵티마이저 설정\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, pretrained_model.parameters()),\n",
        "    lr=CFG['LEARNING_RATE'],\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "scaler = GradScaler()  # AMP용 스케일러\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
        "\n",
        "# 모델 파인튜닝\n",
        "pretrained_model.train()\n",
        "best_loss = float('inf')\n",
        "\n",
        "# GPU 메모리 릴리스\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory has been released.\")\n",
        "\n",
        "for epoch in range(CFG['EPOCHS']):\n",
        "    pretrained_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for noisy_images, clean_images in train_loader:\n",
        "        noisy_images = noisy_images.to(device)\n",
        "        clean_images = clean_images.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 자동 혼합 정밀도(AMP) 사용\n",
        "        with autocast():\n",
        "            outputs = pretrained_model(noisy_images)\n",
        "            loss = criterion(outputs, clean_images)\n",
        "\n",
        "        # 역전파 및 최적화\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, pretrained_model.parameters()), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * noisy_images.size(0)\n",
        "\n",
        "    scheduler.step()  # 학습률 스케줄러 업데이트\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    # 에포크 소요 시간 계산\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    minutes = int(epoch_time // 60)\n",
        "    seconds = int(epoch_time % 60)\n",
        "    hours = int(minutes // 60)\n",
        "    minutes = int(minutes % 60)\n",
        "\n",
        "    # 에포크 손실 및 학습률 출력\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Loss: {epoch_loss:.4f}, LR: {current_lr:.8f}\")\n",
        "    print(f\"Epoch Time: {hours}h {minutes}m {seconds}s\")\n",
        "\n",
        "    # 최상의 모델 저장\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(pretrained_model.state_dict(), 'finetuned_Restormer_Layerfreeze.pth')\n",
        "        print(f\"Best model saved at epoch {epoch+1}\")\n",
        "\n",
        "# 전체 학습 소요 시간 계산\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "minutes = int(training_time // 60)\n",
        "seconds = int(training_time % 60)\n",
        "hours = int(minutes // 60)\n",
        "minutes = int(minutes % 60)\n",
        "\n",
        "# 전체 훈련 시간 출력\n",
        "print(f\"Total Training Time: {hours}h {minutes}m {seconds}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OFWUE9H__jY",
        "outputId": "0bb05aa2-00ed-46b4-daea-e2e64ad31ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Size: 13921\n",
            "Checkpoint loaded successfully\n",
            "GPU memory has been released.\n",
            "Epoch 1/5, Loss: -14.1384, LR: 0.00027135\n",
            "Epoch Time: 0h 6m 52s\n",
            "Best model saved at epoch 1\n",
            "Epoch 2/5, Loss: -15.9794, LR: 0.00019635\n",
            "Epoch Time: 0h 5m 30s\n",
            "Best model saved at epoch 2\n",
            "Epoch 3/5, Loss: -17.0297, LR: 0.00010365\n",
            "Epoch Time: 0h 5m 30s\n",
            "Best model saved at epoch 3\n",
            "Epoch 4/5, Loss: -17.7327, LR: 0.00002865\n",
            "Epoch Time: 0h 5m 30s\n",
            "Best model saved at epoch 4\n",
            "Epoch 5/5, Loss: -18.4395, LR: 0.00000000\n",
            "Epoch Time: 0h 5m 30s\n",
            "Best model saved at epoch 5\n",
            "Total Training Time: 0h 28m 57s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning - Decoder Tuning\n"
      ],
      "metadata": {
        "id": "-o7sxQn8o5Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# 시작 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "# 데이터셋 경로\n",
        "noisy_image_paths = '/content/drive/MyDrive/event/Training/noisy'\n",
        "clean_image_paths = '/content/drive/MyDrive/event/Training/clean'\n",
        "\n",
        "# 데이터 전처리 설정\n",
        "train_transform = Compose([\n",
        "    transforms.Resize((128, 128)),  # 이미지 크기 줄이기\n",
        "    ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 데이터셋 및 데이터 로더 설정\n",
        "train_dataset = CustomDataset(clean_image_paths, noisy_image_paths, transform=train_transform)\n",
        "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
        "\n",
        "# 데이터 로더 설정\n",
        "num_cores = os.cpu_count()\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], num_workers=int(num_cores/2), shuffle=True)\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "pretrained_model = PretrainedRestormer().to(device)\n",
        "\n",
        "# 사전학습된 가중치 로드\n",
        "def load_pretrained_checkpoint(filepath, model):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    state_dict = checkpoint.get('params') or checkpoint.get('state_dict') or checkpoint\n",
        "\n",
        "    # 입력 채널이 불일치하는 경우 첫 번째 레이어 가중치 수정\n",
        "    if model.patch_embed.proj.weight.shape[1] != state_dict['patch_embed.proj.weight'].shape[1]:\n",
        "        print(\"Adjusting input channels to match model configuration\")\n",
        "        original_weight = state_dict['patch_embed.proj.weight']\n",
        "        new_weight = original_weight[:, :3, :, :]  # 첫 3채널만 가져오기 (RGB)\n",
        "        state_dict['patch_embed.proj.weight'] = new_weight\n",
        "\n",
        "    # 가중치 로드\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Checkpoint loaded successfully\")\n",
        "\n",
        "pretrained_checkpoint_path = \"./pretrained_models/single_image_defocus_deblurring.pth\"\n",
        "load_pretrained_checkpoint(pretrained_checkpoint_path, pretrained_model)\n",
        "\n",
        "# 특정 레이어만 파인튜닝 설정\n",
        "for param in pretrained_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in pretrained_model.decoder_level1.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in pretrained_model.refinement.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 손실 함수, 옵티마이저, 학습률 스케줄러 설정\n",
        "def psnr_loss(pred, target, max_val=1.0):\n",
        "    mse = torch.mean((pred - target) ** 2)\n",
        "    return -10 * torch.log10(mse + 1e-10) + max_val\n",
        "\n",
        "criterion = lambda pred, target: nn.L1Loss()(pred, target) - psnr_loss(pred, target)\n",
        "# criterion = nn.L1Loss()\n",
        "\n",
        "# 필요한 부분만 업데이트하도록 옵티마이저 설정\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, pretrained_model.parameters()),\n",
        "    lr=CFG['LEARNING_RATE'],\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "scaler = GradScaler()  # AMP용 스케일러\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
        "\n",
        "# 파라미터 수 출력\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_parameters = count_parameters(pretrained_model)\n",
        "print(\"Total Parameters:\", total_parameters)\n",
        "\n",
        "# 모델 파인튜닝\n",
        "pretrained_model.train()\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(CFG['EPOCHS']):\n",
        "    pretrained_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for noisy_images, clean_images in train_loader:\n",
        "        noisy_images = noisy_images.to(device)\n",
        "        clean_images = clean_images.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 자동 혼합 정밀도(AMP) 사용\n",
        "        with autocast():\n",
        "            outputs = pretrained_model(noisy_images)\n",
        "            loss = criterion(outputs, clean_images)\n",
        "\n",
        "        # 역전파 및 최적화\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, pretrained_model.parameters()), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * noisy_images.size(0)\n",
        "\n",
        "    scheduler.step()  # 학습률 스케줄러 업데이트\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    # 에포크 소요 시간 계산\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    minutes = int(epoch_time // 60)\n",
        "    seconds = int(epoch_time % 60)\n",
        "    hours = int(minutes // 60)\n",
        "    minutes = int(minutes % 60)\n",
        "\n",
        "    # 에포크 손실 및 학습률 출력\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Loss: {epoch_loss:.4f}, LR: {current_lr:.8f}\")\n",
        "    print(f\"Epoch Time: {hours}h {minutes}m {seconds}s\")\n",
        "\n",
        "    # 최상의 모델 저장\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(pretrained_model.state_dict(), 'finetuned_Restormer_decoder_refinement.pth')\n",
        "        print(f\"Best model saved at epoch {epoch+1}\")\n",
        "\n",
        "# 전체 학습 소요 시간 출력\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "minutes = int(training_time // 60)\n",
        "seconds = int(training_time % 60)\n",
        "hours = int(minutes // 60)\n",
        "minutes = int(minutes % 60)\n",
        "\n",
        "print(f\"Total Training Time: {hours}h {minutes}m {seconds}s\")\n"
      ],
      "metadata": {
        "id": "vJRrS7ThpN3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d142458-5506-4e5b-c074-23dbeda65e8e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Size: 13921\n",
            "Checkpoint loaded successfully\n",
            "Total Parameters: 942968\n",
            "Epoch 1/5, Loss: -13.2496, LR: 0.00027135\n",
            "Epoch Time: 0h 8m 56s\n",
            "Best model saved at epoch 1\n",
            "Epoch 2/5, Loss: -14.0997, LR: 0.00019635\n",
            "Epoch Time: 0h 3m 37s\n",
            "Best model saved at epoch 2\n",
            "Epoch 3/5, Loss: -14.4894, LR: 0.00010365\n",
            "Epoch Time: 0h 3m 37s\n",
            "Best model saved at epoch 3\n",
            "Epoch 4/5, Loss: -14.8983, LR: 0.00002865\n",
            "Epoch Time: 0h 3m 37s\n",
            "Best model saved at epoch 4\n",
            "Epoch 5/5, Loss: -15.1820, LR: 0.00000000\n",
            "Epoch Time: 0h 3m 36s\n",
            "Best model saved at epoch 5\n",
            "Total Training Time: 0h 24m 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "XEzblPWVsKVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class CustomDatasetVal(data.Dataset):\n",
        "    def __init__(self, noisy_image_paths, clean_image_paths, transform=None):\n",
        "        self.noisy_image_paths = [os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
        "        self.clean_image_paths = [os.path.join(clean_image_paths, x) for x in os.listdir(clean_image_paths)]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.noisy_image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        noisy_image_path = self.noisy_image_paths[index]\n",
        "        clean_image_path = self.clean_image_paths[index]\n",
        "\n",
        "        noisy_image = load_img(noisy_image_path)\n",
        "        clean_image = load_img(clean_image_path)\n",
        "\n",
        "        # Convert numpy array to PIL image\n",
        "        if isinstance(noisy_image, np.ndarray):\n",
        "            noisy_image = Image.fromarray(noisy_image)\n",
        "        if isinstance(clean_image, np.ndarray):\n",
        "            clean_image = Image.fromarray(clean_image)\n",
        "\n",
        "        if self.transform:\n",
        "            noisy_image = self.transform(noisy_image)\n",
        "            clean_image = self.transform(clean_image)\n",
        "\n",
        "        return noisy_image, clean_image\n",
        "\n",
        "def calculate_psnr(img1, img2, epsilon=1e-10):\n",
        "    mse = F.mse_loss(img1, img2)\n",
        "    psnr = 20 * torch.log10(1.0 / (torch.sqrt(mse) + epsilon))\n",
        "    return psnr\n",
        "\n",
        "# Checkpoint 로드 함수 수정\n",
        "def load_pretrained_checkpoint(filepath, model):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    state_dict = checkpoint.get('params') or checkpoint.get('state_dict') or checkpoint\n",
        "\n",
        "    # 입력 채널이 불일치하는 경우 첫 번째 레이어 가중치 수정\n",
        "    if model.patch_embed.proj.weight.shape[1] != state_dict['patch_embed.proj.weight'].shape[1]:\n",
        "        print(\"Adjusting input channels to match model configuration\")\n",
        "        original_weight = state_dict['patch_embed.proj.weight']\n",
        "        new_weight = original_weight[:, :3, :, :]  # 첫 3채널만 가져오기 (RGB)\n",
        "        state_dict['patch_embed.proj.weight'] = new_weight\n",
        "\n",
        "    # 가중치 로드\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Checkpoint loaded successfully\")\n",
        "\n",
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    print(\"Checkpoint loaded successfully\")\n",
        "\n",
        "# Validation 성능 평가 함수\n",
        "def validate(model, val_loader, criterion, epsilon=1e-10):\n",
        "    model.eval()\n",
        "    psnr_values = []\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for noisy_images, clean_images in val_loader:\n",
        "            noisy_images = noisy_images.to(device)\n",
        "            clean_images = clean_images.to(device)\n",
        "\n",
        "            outputs = model(noisy_images)\n",
        "            loss = criterion(outputs, clean_images)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # PSNR 계산\n",
        "            outputs = outputs.clamp(0, 1)  # 클램핑\n",
        "            clean_images = clean_images.clamp(0, 1)  # 클램핑\n",
        "            psnr = calculate_psnr(outputs, clean_images, epsilon)\n",
        "            psnr_values.append(psnr.item())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "    return avg_val_loss, avg_psnr\n"
      ],
      "metadata": {
        "id": "uJ4WASjpsNHM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Model"
      ],
      "metadata": {
        "id": "PK8YrbcTcQI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 변환 정의\n",
        "val_transform = Compose([\n",
        "    ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 데이터셋 경로\n",
        "noisy_val_image_paths = '/content/drive/MyDrive/event/Validation/noisy'\n",
        "clean_val_image_paths = '/content/drive/MyDrive/event/Validation/clean'\n",
        "\n",
        "# Validation 데이터셋 로드 및 전처리\n",
        "val_dataset = CustomDataset(clean_val_image_paths, noisy_val_image_paths, transform=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "print(f\"Validation Dataset Size: {len(val_dataset)}\")\n",
        "\n",
        "# 모델의 체크포인트 경로\n",
        "finetuned_model_path = 'fine_tuned_PSNRloss_15epoch.pth'\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Restormer 모델 생성 - 논문 설정 적용\n",
        "finetuned_model = PretrainedRestormer().to(device)\n",
        "\n",
        "# 모델을 평가 모드로 전환\n",
        "finetuned_model.eval()\n",
        "\n",
        "# Checkpoint 로드\n",
        "load_pretrained_checkpoint(finetuned_model_path, finetuned_model)\n",
        "\n",
        "# Validation 실행\n",
        "avg_val_loss, avg_psnr = validate(finetuned_model, val_loader, criterion)\n",
        "print(f\"Validation Loss: {avg_val_loss:.4f}, Validation PSNR: {avg_psnr:.2f} dB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGznNsY3sY0u",
        "outputId": "b596a63c-9ad2-45ed-e8c6-3f6b86014844"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Dataset Size: 183\n",
            "Checkpoint loaded successfully\n",
            "Validation Loss: -17.0970, Validation PSNR: 38.59 dB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hZsRt57R84a"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Model"
      ],
      "metadata": {
        "id": "itCVEjsci36_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Y9AOra8RR84a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cdc55cc-0199-4ae4-ac54-31b6c3dfdd9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded successfully\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_014.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_015.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_029.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_007.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_013.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_039.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_004.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_011.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_001.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_012.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_028.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_038.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_005.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_006.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_010.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_159.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_149.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_161.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_158.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_148.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_160.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_204.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_170.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_199.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_198.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_211.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_164.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_203.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_205.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_167.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_171.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_166.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_207.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_202.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_175.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_173.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_172.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_174.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_165.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_206.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_210.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_201.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_163.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_177.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_189.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_162.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_176.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_016.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_200.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_002.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_000.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_118.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_130.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_124.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_131.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_079.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_119.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_051.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_045.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_125.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_086.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_092.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_060.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_048.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_074.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_129.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_128.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_115.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_114.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_117.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_088.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_102.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_075.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_077.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_061.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_103.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_062.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_076.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_071.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_105.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_139.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_067.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_059.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_098.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_111.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_063.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_100.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_049.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_073.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_106.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_066.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_112.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_099.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_089.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_107.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_116.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_072.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_113.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_101.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_017.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_138.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_064.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_003.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_188.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_070.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_058.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_104.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_065.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_110.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_019.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_184.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_027.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_153.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_025.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_033.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_147.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_190.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_152.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_146.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_026.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_185.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_191.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_041.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_096.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_055.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_082.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_032.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_095.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_122.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_081.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_123.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_056.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_137.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_042.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_120.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_069.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_097.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_136.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_126.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_047.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_053.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_084.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_090.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_133.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_094.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_080.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_127.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_083.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_040.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_134.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_054.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_068.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_135.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_108.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_109.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_057.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_121.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_043.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_091.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_085.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_050.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_087.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_046.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_052.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_078.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_093.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_044.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_132.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_036.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_208.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_022.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_181.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_195.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_142.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_157.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_023.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_180.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_143.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_194.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_178.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_187.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_193.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_179.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_145.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_186.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_150.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_144.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_168.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_209.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_037.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_140.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_196.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_141.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_035.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_009.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_182.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_021.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_169.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_155.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_008.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_197.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_024.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_154.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_018.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_020.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_183.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_034.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_030.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_156.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_151.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_031.jpg\n",
            "Saved denoised image: /content/drive/MyDrive/open 2/test/submission/TEST_192.jpg\n"
          ]
        }
      ],
      "source": [
        "class CustomDatasetTest(data.Dataset):\n",
        "    def __init__(self, noisy_image_paths, transform=None):\n",
        "        self.noisy_image_paths = [os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.noisy_image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        noisy_image_path = self.noisy_image_paths[index]\n",
        "        noisy_image = load_img(self.noisy_image_paths[index])\n",
        "\n",
        "        # Convert numpy array to PIL image\n",
        "        if isinstance(noisy_image, np.ndarray):\n",
        "            noisy_image = Image.fromarray(noisy_image)\n",
        "\n",
        "        if self.transform:\n",
        "            noisy_image = self.transform(noisy_image)\n",
        "\n",
        "        return noisy_image, noisy_image_path\n",
        "\n",
        "# 모델의 체크포인트 경로\n",
        "finetuned_model_path = 'fine_tuned_PSNRloss_15epoch.pth'\n",
        "\n",
        "# Restormer 모델 생성 - 논문 설정 적용\n",
        "finetuned_model = PretrainedRestormer().to(device)\n",
        "\n",
        "# 모델을 평가 모드로 전환\n",
        "finetuned_model.eval()\n",
        "\n",
        "# Checkpoint 로드\n",
        "load_pretrained_checkpoint(finetuned_model_path, finetuned_model)\n",
        "\n",
        "test_transform = Compose([\n",
        "    ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "def load_img(filepath):\n",
        "    img = cv2.imread(filepath)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "# model = Restormer()\n",
        "# model.load_state_dict(torch.load(pretrained_model_path))\n",
        "\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "finetuned_model.to(device)\n",
        "\n",
        "\n",
        "# 데이터셋 경로\n",
        "test_data_path = '/content/drive/MyDrive/open 2/test/Input'\n",
        "output_path = '/content/drive/MyDrive/open 2/test/submission'\n",
        "\n",
        "# 데이터셋 로드 및 전처리\n",
        "test_dataset = CustomDatasetTest(test_data_path, transform=test_transform)\n",
        "\n",
        "# 데이터 로더 설정\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "\n",
        "# 이미지 denoising 및 저장\n",
        "for noisy_image, noisy_image_path in test_loader:\n",
        "    noisy_image = noisy_image.to(device)\n",
        "    denoised_image = finetuned_model(noisy_image)\n",
        "\n",
        "    # denoised_image를 CPU로 이동하여 이미지 저장\n",
        "    denoised_image = denoised_image.cpu().squeeze(0)\n",
        "    denoised_image = (denoised_image * 0.5 + 0.5).clamp(0, 1)\n",
        "    denoised_image = transforms.ToPILImage()(denoised_image)\n",
        "\n",
        "    # Save denoised image\n",
        "    output_filename = noisy_image_path[0]\n",
        "    denoised_filename = output_path + '/' + output_filename.split('/')[-1][:-4] + '.jpg'\n",
        "    denoised_image.save(denoised_filename)\n",
        "\n",
        "    print(f'Saved denoised image: {denoised_filename}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 메모리 릴리스\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory has been released.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niIRiHrNjktq",
        "outputId": "36413464-5794-4629-d4fb-42c6488f699e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory has been released.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbtCEZ49R84b"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8ltZQsr6R84b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba31a4d-9064-4da4-f863-0dd1ea92a160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created ./submission.zip successfully.\n"
          ]
        }
      ],
      "source": [
        "def zip_folder(folder_path, output_zip):\n",
        "    shutil.make_archive(output_zip, 'zip', folder_path)\n",
        "    print(f\"Created {output_zip}.zip successfully.\")\n",
        "\n",
        "zip_folder(output_path, './submission')"
      ]
    }
  ]
}